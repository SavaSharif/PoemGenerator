{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Sava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk as nl\n",
    "from sklearn import naive_bayes\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "\n",
    "nl.download('punkt')\n",
    "nl.download('averaged_perceptron_tagger')\n",
    "nl.download('cmudict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8472, 3)\n",
      "                            0                                        1  \\\n",
      "134123   [i, have, a, really]        [cute, outfit, for, tonight, but]   \n",
      "134124   [we, can, hold, our]      [hands, together, through, the, oh]   \n",
      "134125   [you, my, hero, man]  [get, well, soon, i, loved, you, since]   \n",
      "134126  [my, biggest, guilty]           [pleasure, is, probably, punk]   \n",
      "134127  [yes, daddy, tie, me]    [up, choke, me, to, dead, and, leave]   \n",
      "\n",
      "                                   2  \n",
      "134123       [i, can, not, find, it]  \n",
      "134124    [oh, through, the, oh, oh]  \n",
      "134125  [here, in, the, real, world]  \n",
      "134126      [covers, of, pop, songs]  \n",
      "134127           [my, body, to, rot]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>joined_sents</th>\n",
       "      <th>sent_len</th>\n",
       "      <th>tagged_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134123</th>\n",
       "      <td>[i, have, a, really]</td>\n",
       "      <td>[cute, outfit, for, tonight, but]</td>\n",
       "      <td>[i, can, not, find, it]</td>\n",
       "      <td>[i, have, a, really, cute, outfit, for, tonigh...</td>\n",
       "      <td>[4, 5, 5]</td>\n",
       "      <td>[(i, NNS), (have, VBP), (a, DT), (really, RB),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134124</th>\n",
       "      <td>[we, can, hold, our]</td>\n",
       "      <td>[hands, together, through, the, oh]</td>\n",
       "      <td>[oh, through, the, oh, oh]</td>\n",
       "      <td>[we, can, hold, our, hands, together, through,...</td>\n",
       "      <td>[4, 5, 5]</td>\n",
       "      <td>[(we, PRP), (can, MD), (hold, VB), (our, PRP$)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134125</th>\n",
       "      <td>[you, my, hero, man]</td>\n",
       "      <td>[get, well, soon, i, loved, you, since]</td>\n",
       "      <td>[here, in, the, real, world]</td>\n",
       "      <td>[you, my, hero, man, get, well, soon, i, loved...</td>\n",
       "      <td>[4, 7, 5]</td>\n",
       "      <td>[(you, PRP), (my, PRP$), (hero, NN), (man, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134126</th>\n",
       "      <td>[my, biggest, guilty]</td>\n",
       "      <td>[pleasure, is, probably, punk]</td>\n",
       "      <td>[covers, of, pop, songs]</td>\n",
       "      <td>[my, biggest, guilty, pleasure, is, probably, ...</td>\n",
       "      <td>[3, 4, 4]</td>\n",
       "      <td>[(my, PRP$), (biggest, JJS), (guilty, JJ), (pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134127</th>\n",
       "      <td>[yes, daddy, tie, me]</td>\n",
       "      <td>[up, choke, me, to, dead, and, leave]</td>\n",
       "      <td>[my, body, to, rot]</td>\n",
       "      <td>[yes, daddy, tie, me, up, choke, me, to, dead,...</td>\n",
       "      <td>[4, 7, 4]</td>\n",
       "      <td>[(yes, RB), (daddy, JJ), (tie, NNS), (me, PRP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0                                        1  \\\n",
       "134123   [i, have, a, really]        [cute, outfit, for, tonight, but]   \n",
       "134124   [we, can, hold, our]      [hands, together, through, the, oh]   \n",
       "134125   [you, my, hero, man]  [get, well, soon, i, loved, you, since]   \n",
       "134126  [my, biggest, guilty]           [pleasure, is, probably, punk]   \n",
       "134127  [yes, daddy, tie, me]    [up, choke, me, to, dead, and, leave]   \n",
       "\n",
       "                                   2  \\\n",
       "134123       [i, can, not, find, it]   \n",
       "134124    [oh, through, the, oh, oh]   \n",
       "134125  [here, in, the, real, world]   \n",
       "134126      [covers, of, pop, songs]   \n",
       "134127           [my, body, to, rot]   \n",
       "\n",
       "                                             joined_sents   sent_len  \\\n",
       "134123  [i, have, a, really, cute, outfit, for, tonigh...  [4, 5, 5]   \n",
       "134124  [we, can, hold, our, hands, together, through,...  [4, 5, 5]   \n",
       "134125  [you, my, hero, man, get, well, soon, i, loved...  [4, 7, 5]   \n",
       "134126  [my, biggest, guilty, pleasure, is, probably, ...  [3, 4, 4]   \n",
       "134127  [yes, daddy, tie, me, up, choke, me, to, dead,...  [4, 7, 4]   \n",
       "\n",
       "                                             tagged_sents  \n",
       "134123  [(i, NNS), (have, VBP), (a, DT), (really, RB),...  \n",
       "134124  [(we, PRP), (can, MD), (hold, VB), (our, PRP$)...  \n",
       "134125  [(you, PRP), (my, PRP$), (hero, NN), (man, NN)...  \n",
       "134126  [(my, PRP$), (biggest, JJS), (guilty, JJ), (pl...  \n",
       "134127  [(yes, RB), (daddy, JJ), (tie, NNS), (me, PRP)...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data\\poems\\haiku.csv')\n",
    "df = df.tail(10000)\n",
    "\n",
    "df = df[['0', '1', '2']]\n",
    "\n",
    "# get the CMU Pronouncing Dictionary\n",
    "transcr = nl.corpus.cmudict.dict()\n",
    "\n",
    "# concatenate data of all columns into one column and word tokenize\n",
    "df['0'] = df['0'].apply(lambda x: [w.lower() for w in nl.tokenize.word_tokenize(x) if w.isalpha()])\n",
    "df['1'] = df['1'].apply(lambda x: [w.lower() for w in nl.tokenize.word_tokenize(x) if w.isalpha()])\n",
    "df['2'] = df['2'].apply(lambda x: [w.lower() for w in nl.tokenize.word_tokenize(x) if w.isalpha()])\n",
    "\n",
    "def get_syllables(word):\n",
    "    if word in transcr:\n",
    "        pron = transcr[word][0]\n",
    "\n",
    "        # get the number of syllables in the word\n",
    "        syllable_count = sum(y.isdigit() for x in pron for y in x)\n",
    "        return syllable_count\n",
    "    return 0\n",
    "\n",
    "# check the amount of syllables in the sentence\n",
    "for ind, row in df.iterrows():\n",
    "    sentence_syl_count = sum(get_syllables(w) for w in row['0'])\n",
    "    sentence_syl_count += sum(get_syllables(w) for w in row['1'])\n",
    "    sentence_syl_count += sum(get_syllables(w) for w in row['2'])\n",
    "\n",
    "    if(sentence_syl_count != 17):\n",
    "        df = df.drop(ind)  # haiku must have 17 syllables\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "print(df.head())\n",
    "# tag the tokenized sentences\n",
    "\n",
    "df['joined_sents'] = df.apply(lambda x: [*x['0'], *x['1'], *x['2']], axis=1)\n",
    "df['sent_len'] = df.apply(lambda x: [len(x['0']), len(x['1']), len(x['2'])], axis=1)\n",
    "\n",
    "\n",
    "df['tagged_sents'] = df['joined_sents'].apply(lambda x: nl.pos_tag(x))\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\n",
    "        \"suffix(1)\": word[-1:],\n",
    "        \"suffix(2)\": word[-2:],\n",
    "        \"suffix(3)\": word[-3:],\n",
    "        \"pos\": tag\n",
    "    }\n",
    "\n",
    "    features[\"syllable_sum\"] = sum(get_syllables(word) for word in sentence[:i])\n",
    "\n",
    "    features.update(get_n_word(sentence, i, -1))\n",
    "    # features.update(get_n_word(sentence, i, -2))\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_n_word(sentence, i, n):\n",
    "    if i + n < 0:\n",
    "        return {\n",
    "            \"SOS\": True\n",
    "        }\n",
    "    return {\n",
    "        f'{n}:word':sentence[i + n][0],\n",
    "        f'{n}:word:pos':sentence[i + n][1],\n",
    "    }\n",
    "    \n",
    "\n",
    "def get_next_word(sentence, i):\n",
    "    if i + 1 >= len(sentence):\n",
    "        return \"<END>\"\n",
    "    return sentence[i+1]\n",
    "\n",
    "featuresets = []\n",
    "for ind, row in df.iterrows():\n",
    "    featuresets.append([])\n",
    "    for i, (word, tag) in enumerate(row['tagged_sents']):\n",
    "        featuresets[-1].append((pos_features(row['tagged_sents'], i), get_next_word(row['joined_sents'], i)))\n",
    "\n",
    "size = int(len(featuresets) * 0.1)\n",
    "\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "f_f = flatten(featuresets)\n",
    "\n",
    "train_set, test_set = f_f[size:], f_f[:size]\n",
    "classifier = nl.NaiveBayesClassifier.train(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = SklearnClassifier(naive_bayes.MultinomialNB()).train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 'PRP'), ('i', 'NN'), ('to', 'TO'), ('<END>', 'NN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prev_word = 'you'\n",
    "postag = nl.tag.pos_tag([prev_word])[0][1]\n",
    "poem = [(prev_word, postag)]\n",
    "\n",
    "i = 0\n",
    "while prev_word != '<END>' and i < 20:\n",
    "    vector = pos_features(poem, i)\n",
    "    prev_word =  classifier2.prob_classify(vector).generate()\n",
    "    postag = nl.tag.pos_tag([prev_word])[0][1]\n",
    "    poem.append((prev_word, postag))\n",
    "    i += 1\n",
    "\n",
    "poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1192443919716647"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl.classify.accuracy(classifier2, test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'want'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.prob_classify(f_f[0][0]).generate()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b4e8bc1b5fff3651ec28fee6c734c37e66079d3dafb6e2969224ed4a1eddf14"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('crea-2-w6Y9zmrs': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
